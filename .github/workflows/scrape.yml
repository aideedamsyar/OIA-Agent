# .github/workflows/scrape.yml

name: Scrape Hanyang OIA Notices

on:
  workflow_dispatch: # Allow manual trigger
  schedule:
    # Run at 00:00 UTC (9 AM KST) and 08:00 UTC (5 PM KST) daily
    - cron: '0 0 * * *'
    - cron: '0 8 * * *'

# Add write permissions for contents (for pushing changes)
permissions:
  contents: write

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
      - name: Check out repository
        uses: actions/checkout@v4
        with:
          fetch-depth: 0 # Fetch all history for pull/rebase

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x' # Use a recent Python 3 version

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run scraper
        run: python scraper_enhanced.py  # Use the enhanced scraper with AI analysis
        env:
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}  # Pass the API key as environment variable

      - name: Commit and push if changed
        run: |
          git config --global user.name "github-actions[bot]"
          git config --global user.email "github-actions[bot]@users.noreply.github.com"
          git add web/src/data/notices.json
          # Check if there are changes staged
          git diff --staged --quiet || git commit -m "Update OIA notices with AI analysis"
          # Skip pull/rebase, just force push if a commit was made
          # Check if the previous command created a commit (check HEAD vs origin/main)
          if ! git diff --quiet HEAD origin/main -- web/src/data/notices.json; then
            echo "Pushing updated notices.json..."
            git push --force origin main
          else
            echo "No changes to notices.json to push."
          fi
      