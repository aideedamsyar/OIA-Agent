# Product Requirements Document: Hanyang OIA Notice Scraper

## 1. Introduction & Goal

This project aims to build a web application that automatically fetches, filters, and displays relevant job and internship opportunities from the Hanyang University Office of International Affairs (OIA) notice board. The goal is to provide students with a centralized and easy-to-access view of career-related announcements, saving them the effort of manually checking the OIA website daily.

## 2. Requirements

### Functional Requirements

*   **F1: Daily Scraping:** The system must automatically scrape the first page of the Hanyang OIA notice board (`https://oia.hanyang.ac.kr/notice`) once per day.
*   **F2: Content Filtering:** The system must parse the scraped content and filter notices based on keywords indicating job or internship opportunities. The primary keywords are: `internship`, `인턴십`, `채용`, `취업`, `모집`. Only notices containing these keywords in their title on the *first page* should be considered relevant.
*   **F3: Data Storage:** Filtered notices (including title, link, date) must be stored persistently.
*   **F4: Web Interface:** A web interface must display the filtered notices in a clean, user-friendly manner.
*   **F5 (Optional): AI Relevance Analysis:** For each filtered notice, fetch the detailed content by following its link. Use an AI service (e.g., Groq's free tier) to analyze the content and determine its relevance more accurately (e.g., distinguishing actual job postings from general recruitment information sessions).
*   **F6 (Optional): Display AI Summary:** If F5 is implemented, display a brief summary or relevance score generated by the AI alongside the notice title.

### Non-Functional Requirements

*   **NF1: Free Hosting:** The entire application (backend scraping logic, data storage, frontend) must be hostable using free service tiers.
*   **NF2: Performance:** The web interface should load quickly. Scraping should be efficient and avoid overloading the source website.
*   **NF3: Reliability:** The scraping process should be robust against minor HTML structure changes if possible, though significant changes might require manual updates.
*   **NF4: UI/UX:** The web interface should have a professional look and feel, potentially using a modern UI library like `shadcn/ui`.

## 3. Proposed Stack & Architecture

*   **Automation Engine:** **GitHub Actions**
    *   *Reasoning:* Provides free compute time for scheduled tasks (cron jobs), perfect for daily scraping. Integrates directly with the code repository.
*   **Scraping Script:** **Python** with `requests` (for fetching HTML) and `BeautifulSoup4` (for parsing HTML).
    *   *Reasoning:* Mature and robust libraries for web scraping. Python is well-suited for scripting and data manipulation. Runs easily within GitHub Actions.
*   **Data Storage:** **JSON file** stored within the GitHub repository.
    *   *Reasoning:* Simplest approach for a small amount of data. Free, version-controlled, and easily accessible by the frontend deployed from the same repository. Avoids the complexity and potential costs/limits of external databases on free tiers.
*   **Frontend Framework:** **Next.js (React)**
    *   *Reasoning:* Excellent for building static and server-rendered applications. Integrates well with Vercel/Netlify for free hosting. Strong community support and works seamlessly with `shadcn/ui`.
*   **UI Library:** **shadcn/ui** + **Tailwind CSS**
    *   *Reasoning:* User request. Provides beautiful, accessible components built on Tailwind CSS, allowing for rapid UI development.
*   **Hosting:** **Vercel** or **Netlify**
    *   *Reasoning:* Offer generous free tiers for hosting static sites and serverless functions (if needed later). Integrate seamlessly with GitHub for continuous deployment.
*   **AI Service (Optional):** **Groq API**
    *   *Reasoning:* User suggestion. Offers a fast inference engine potentially available with a free tier suitable for analyzing a small number of posts daily. Requires API key management.

### Architecture Overview

1.  **GitHub Action (Scheduled Daily):**
    *   Checks out the repository.
    *   Runs the Python scraping script.
2.  **Python Scraper:**
    *   Fetches HTML from `https://oia.hanyang.ac.kr/notice`.
    *   Parses the HTML using BeautifulSoup to find the notice table.
    *   Iterates through rows on the first page.
    *   Filters rows based on keywords in the title.
    *   *(Optional AI Step)*: For each filtered notice, fetches the notice detail page URL. Sends the content to Groq API for analysis/summarization.
    *   Formats the filtered data (title, link, date, optional AI summary) into a structured list.
    *   Overwrites/updates the `notices.json` file in the repository.
    *   Commits and pushes the updated `notices.json` file back to the repository.
3.  **Next.js Frontend (Hosted on Vercel/Netlify):**
    *   When a user visits the site, the Next.js application fetches the `notices.json` file (statically at build time or client-side).
    *   Renders the list of notices using React components styled with `shadcn/ui` and Tailwind CSS.
4.  **Deployment:** The push to the main branch (triggered by the GitHub Action updating `notices.json` or by code changes) automatically triggers a new deployment on Vercel/Netlify.

## 4. Risks & Challenges

*   **Website Structure Changes:** The OIA website's HTML structure may change, breaking the scraper. Requires monitoring and potential script updates.
*   **Scraping Blocks:** The university might implement measures to block scraping (e.g., IP bans, CAPTCHAs). The script should use appropriate headers (User-Agent) and potentially delays to minimize risk.
*   **Terms of Service:** Verify Hanyang OIA's terms of service regarding automated scraping. Proceed ethically and avoid excessive requests.
*   **AI Costs/Limits:** If implementing the optional AI feature, monitor Groq API usage to stay within free tier limits. API keys need secure management (e.g., GitHub Secrets).
*   **Keyword Accuracy:** The initial keyword filtering might include irrelevant posts (e.g., a general recruitment *information session* vs. a specific job *posting*). The optional AI step aims to mitigate this.

## 5. Future Considerations

*   Scraping multiple pages.
*   User accounts for saving/tracking notices.
*   Email/push notifications for new relevant posts.
*   More sophisticated relevance filtering (NLP without external AI).
*   Historical data tracking.

## 6. Implementation Plan

This project will be implemented in the following phases:

**Phase 1: Core Scraper Development**
*   [x] Finalize and test the Python script (`scraper.py`) to fetch the main notice page.
*   [x] Implement HTML parsing logic using BeautifulSoup to extract all notices (title, link, date) from the first page table.
*   [x] Implement keyword filtering logic.
*   [x] Implement logic to save the filtered notices to `notices.json` with a timestamp.
*   [x] Thoroughly test the scraper locally with the target URL.

**Phase 2: Automation Setup**
*   [ ] Create a GitHub Actions workflow file (`.github/workflows/scrape.yml`).
*   [ ] Configure the workflow to run on a daily schedule (e.g., `cron: '0 0 * * *'` for midnight UTC).
*   [ ] Add steps to check out the code, set up Python, install dependencies (`requests`, `beautifulsoup4`).
*   [ ] Add a step to execute `scraper.py`.
*   [ ] Add steps to commit and push the updated `notices.json` back to the repository if changes are detected.
*   [ ] Test the workflow manually and verify scheduled runs.

**Phase 3: Frontend Project Setup**
*   [ ] Initialize a new Next.js project (using `create-next-app`).
*   [ ] Integrate Tailwind CSS.
*   [ ] Set up `shadcn/ui` following its documentation.
*   [ ] Create the basic layout and page structure (e.g., `app/page.tsx`).

**Phase 4: Frontend Notice Display**
*   [ ] Implement logic to fetch or read the `notices.json` data within the Next.js application (Static Site Generation (SSG) preferred for performance).
*   [ ] Design the notice display using `shadcn/ui` components (e.g., `<Card>` for each notice or a `<Table>`).
*   [ ] Display the notice title, date, and provide a clickable link to the original notice URL.
*   [ ] Add styling for a clean and professional appearance.
*   [ ] Display the "Last Updated" timestamp from `notices.json`.

**Phase 5: Deployment**
*   [ ] Create a new project on Vercel (or Netlify).
*   [ ] Link the Vercel project to the GitHub repository.
*   [ ] Configure build settings (should work automatically for Next.js).
*   [ ] Trigger the first deployment.
*   [ ] Verify that pushes to the main branch (including those by the GitHub Action) trigger automatic redeployments.

**Phase 6: (Optional) AI Relevance Analysis**
*   [ ] If pursuing, obtain a Groq API key and add it as a GitHub Secret.
*   [ ] Modify `scraper.py`:
    *   Add a function to fetch the content of an individual notice page given its URL.
    *   Add HTML parsing logic to extract the main text content from the notice detail page.
    *   Integrate the Groq Python client or use `requests` to call the Groq API endpoint.
    *   Pass the extracted text to Groq with a prompt asking for relevance analysis or summarization focused on job/internship details.
    *   Retrieve the AI response and add it to the notice data in `notices.json`.
    *   Handle potential errors during fetching or AI analysis.
*   [ ] Modify the Next.js frontend:
    *   Update the data fetching/reading logic to include the new AI field.
    *   Display the AI-generated summary or relevance score alongside the other notice details. 